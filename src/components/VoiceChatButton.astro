---
// VoiceChatButton component for Vapi integration
// Users need to set their VAPI_PUBLIC_KEY and VAPI_ASSISTANT_ID
// These can be set as environment variables or passed as props
import { Mic } from "lucide-astro";

interface Props {
  publicKey?: string;
  assistantId?: string;
  position?:
    | "bottom-right"
    | "bottom-left"
    | "top-right"
    | "top-left"
    | "center-bottom";
  // Voice recognition configuration
  enableVoiceRecognition?: boolean;
  voiceProfileId?: string;
  assistantOverrides?: Record<string, any>;
  // Authentication
  userId?: string;
  isAuthenticated?: boolean;
}

const {
  publicKey = import.meta.env.PUBLIC_VAPI_PUBLIC_KEY || "",
  assistantId = import.meta.env.PUBLIC_VAPI_ASSISTANT_ID || "",
  position = "center-bottom",
  enableVoiceRecognition = import.meta.env
    .PUBLIC_VAPI_ENABLE_VOICE_RECOGNITION === "true",
  voiceProfileId = import.meta.env.PUBLIC_VAPI_VOICE_PROFILE_ID || "",
  assistantOverrides = {},
  userId,
  isAuthenticated = false,
} = Astro.props;

const positionClasses = {
  "bottom-right": "bottom-right",
  "bottom-left": "bottom-left",
  "top-right": "top-right",
  "top-left": "top-left",
  "center-bottom": "center-bottom",
};
---

<div
  id="vapi-widget-container"
  class={`vapi-widget-container ${positionClasses[position]}`}
>
  <button
    id="vapi-voice-button"
    class="vapi-voice-button"
    aria-label="Start voice chat"
    title="Click to speak with our AI assistant"
  >
    <!-- Toggle switch track -->
    <span class="toggle-track"></span>

    <!-- Toggle switch thumb -->
    <span class="toggle-thumb">
      <!-- Microphone icon (active/listening state) -->
      <Mic
        id="mic-icon"
        class="vapi-icon icon-active"
        size={20}
        stroke-width={2.5}
        style="display: none;"
      />

      <!-- Loading spinner (connecting state) -->
      <div id="loading-spinner" class="loading-spinner" style="display: none;">
        <div class="spinner-ring"></div>
      </div>
    </span>
  </button>

  <!-- Audio level meter -->
  <div id="audio-level-meter" class="audio-level-meter" style="display: none;">
    <div class="level-bars">
      <div class="level-bar" style="--delay: 0ms"></div>
      <div class="level-bar" style="--delay: 50ms"></div>
      <div class="level-bar" style="--delay: 100ms"></div>
      <div class="level-bar" style="--delay: 150ms"></div>
      <div class="level-bar" style="--delay: 200ms"></div>
    </div>
  </div>
</div>

<script
  define:vars={{
    publicKey,
    assistantId,
    enableVoiceRecognition,
    voiceProfileId,
    assistantOverrides,
  }}
>
  // Import Clerk client stores
  import { $authStore, $userStore, $isLoadedStore } from "@clerk/astro/client";

  // Get auth state from Clerk client-side using nanostores
  let userId = null;
  let isAuthenticated = false;
  let unsubscribeAuth = null;
  let unsubscribeUser = null;

  // Initialize Clerk auth state by subscribing to stores
  function initAuth() {
    try {
      // Subscribe to auth store for userId and session info
      unsubscribeAuth = $authStore.subscribe((auth) => {
        if (auth && auth.userId) {
          userId = auth.userId;
          isAuthenticated = true;
          console.log("Clerk auth store updated - user authenticated:", userId);
        } else {
          userId = null;
          isAuthenticated = false;
          console.log("Clerk auth store updated - user not authenticated");
        }
      });

      // Also subscribe to user store for additional user data
      unsubscribeUser = $userStore.subscribe((user) => {
        if (user && user.id) {
          userId = user.id;
          isAuthenticated = true;
          console.log("Clerk user store updated - user:", user.id);
        }
      });

      // Check initial state
      const initialAuth = $authStore.get();
      console.log("Initial Clerk auth store:", initialAuth);
      if (initialAuth && initialAuth.userId) {
        userId = initialAuth.userId;
        isAuthenticated = true;
        console.log("Initial Clerk auth state - user authenticated:", userId);
      }

      const initialUser = $userStore.get();
      console.log("Initial Clerk user store:", initialUser);
      if (initialUser && initialUser.id) {
        userId = initialUser.id;
        isAuthenticated = true;
        console.log("Initial Clerk user state - user:", userId);
      }

      // #region agent log
      fetch(
        "http://127.0.0.1:7242/ingest/b70a058c-165e-4820-adda-384130e4a687",
        {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({
            location: "VoiceChatButton.astro:180",
            message: "Clerk auth initialization",
            data: {
              initialAuth: initialAuth
                ? { userId: initialAuth.userId, hasUser: !!initialAuth.user }
                : null,
              initialUser: initialUser ? { id: initialUser.id } : null,
              isAuthenticated,
              userId: userId || "not set",
            },
            timestamp: Date.now(),
            sessionId: "debug-session",
            runId: "init",
            hypothesisId: "A",
          }),
        }
      ).catch(() => {});
      // #endregion
    } catch (error) {
      console.warn("Clerk auth initialization error:", error);
    }
  }

  // Initialize auth when script loads
  if (typeof window !== "undefined") {
    if (document.readyState === "loading") {
      document.addEventListener("DOMContentLoaded", initAuth);
    } else {
      initAuth();
    }
  }

  // Vapi SDK will be attached to window.Vapi
  console.log("Vapi credentials check:", {
    publicKey: publicKey ? "***" + publicKey.slice(-4) : "MISSING",
    assistantId: assistantId ? assistantId : "MISSING",
    publicKeyLength: publicKey?.length || 0,
    assistantIdLength: assistantId?.length || 0,
  });

  // #region agent log
  fetch("http://127.0.0.1:7242/ingest/b70a058c-165e-4820-adda-384130e4a687", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      location: "VoiceChatButton.astro:100",
      message: "Voice recognition config loaded",
      data: {
        enableVoiceRecognition,
        voiceProfileId: voiceProfileId || "not set",
        assistantOverridesKeys: Object.keys(assistantOverrides || {}),
      },
      timestamp: Date.now(),
      sessionId: "debug-session",
      runId: "init",
      hypothesisId: "A",
    }),
  }).catch(() => {});
  // #endregion

  console.log("=== Voice Recognition Configuration Debug ===");
  console.log(
    "enableVoiceRecognition:",
    enableVoiceRecognition,
    "(type:",
    typeof enableVoiceRecognition,
    ")"
  );
  console.log(
    "voiceProfileId:",
    voiceProfileId || "not set",
    "(type:",
    typeof voiceProfileId,
    ")"
  );
  console.log("assistantOverrides:", assistantOverrides);
  console.log("=== Authentication Debug ===");
  console.log(
    "isAuthenticated:",
    isAuthenticated,
    "(type:",
    typeof isAuthenticated,
    ")"
  );
  console.log("userId:", userId || "not set", "(type:", typeof userId, ")");

  // #region agent log
  fetch("http://127.0.0.1:7242/ingest/b70a058c-165e-4820-adda-384130e4a687", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      location: "VoiceChatButton.astro:125",
      message: "Detailed voice recognition config check",
      data: {
        enableVoiceRecognition,
        enableVoiceRecognitionType: typeof enableVoiceRecognition,
        voiceProfileId: voiceProfileId || "not set",
        voiceProfileIdType: typeof voiceProfileId,
        assistantOverridesKeys: Object.keys(assistantOverrides || {}),
        assistantOverridesEmpty:
          Object.keys(assistantOverrides || {}).length === 0,
      },
      timestamp: Date.now(),
      sessionId: "debug-session",
      runId: "init",
      hypothesisId: "E",
    }),
  }).catch(() => {});
  // #endregion

  if (!publicKey || !assistantId) {
    console.warn(
      "Vapi credentials not found. Please set PUBLIC_VAPI_PUBLIC_KEY and PUBLIC_VAPI_ASSISTANT_ID environment variables."
    );
  }

  // Lazy load Vapi SDK only when button is clicked (resource optimization)
  let vapiSDKLoaded = false;
  let vapiSDKLoading = false;

  async function loadVapiSDK() {
    const win = window;

    // Return if already loaded
    if (win.Vapi && typeof win.Vapi === "function") {
      return win.Vapi;
    }

    // Return if already loading
    if (vapiSDKLoading) {
      // Wait for loading to complete
      return new Promise((resolve) => {
        const checkInterval = setInterval(() => {
          if (win.Vapi && typeof win.Vapi === "function") {
            clearInterval(checkInterval);
            resolve(win.Vapi);
          } else if (vapiSDKLoaded && !win.Vapi) {
            clearInterval(checkInterval);
            resolve(null);
          }
        }, 100);
      });
    }

    vapiSDKLoading = true;

    try {
      console.log("=== Loading Vapi SDK ===");
      // Try importing from npm package (Vite will bundle it)
      // Use the package name - Vite handles the resolution
      let VapiModule;
      try {
        console.log("Attempting to import @vapi-ai/web from npm...");
        VapiModule = await import("@vapi-ai/web");
        console.log("Local import successful");
      } catch (localError) {
        // If local import fails (expected in browser), use CDN
        // This is normal - browser scripts can't resolve npm packages directly
        console.log("Local import failed, using CDN:", localError.message);
        console.log(
          "Importing from CDN: https://cdn.jsdelivr.net/npm/@vapi-ai/web@latest/+esm"
        );
        VapiModule =
          await import("https://cdn.jsdelivr.net/npm/@vapi-ai/web@latest/+esm");
        console.log("CDN import successful");
      }

      console.log("VapiModule:", VapiModule);
      console.log("VapiModule keys:", Object.keys(VapiModule));
      console.log("VapiModule.default:", VapiModule.default);
      console.log("VapiModule.default type:", typeof VapiModule.default);

      // Handle CommonJS default export
      let Vapi = VapiModule.default;
      console.log("Initial Vapi (from default):", Vapi);
      console.log("Initial Vapi type:", typeof Vapi);

      // If default is not a constructor, try other exports
      if (!Vapi || typeof Vapi !== "function") {
        console.log("Default is not a function, trying other exports...");
        Vapi = VapiModule.Vapi || VapiModule.VapiClient || VapiModule;
        console.log("Vapi after fallback:", Vapi);
        console.log("Vapi type after fallback:", typeof Vapi);
      }

      // Final check - if still not a function, it might be wrapped
      if (!Vapi || typeof Vapi !== "function") {
        // Try accessing the constructor property if it exists
        if (VapiModule.default && VapiModule.default.default) {
          Vapi = VapiModule.default.default;
        }
      }

      // Check if it's actually a constructor
      if (Vapi && typeof Vapi === "function") {
        console.log("Vapi constructor validated successfully");
        console.log("Vapi prototype:", Vapi.prototype);
        console.log(
          "Vapi prototype methods:",
          Object.getOwnPropertyNames(Vapi.prototype)
        );
        win.Vapi = Vapi;
        vapiSDKLoaded = true;
        vapiSDKLoading = false;
        console.log("Vapi SDK loaded and ready");
        return Vapi;
      } else {
        console.error("Vapi module structure:", VapiModule);
        console.error("Final Vapi value:", Vapi);
        console.error("Final Vapi type:", typeof Vapi);
        throw new Error(
          "Vapi is not a constructor. Module type: " + typeof Vapi
        );
      }
    } catch (error) {
      console.error("Failed to load Vapi SDK:", error);
      vapiSDKLoaded = true;
      vapiSDKLoading = false;
      return null;
    }
  }

  // Audio level analyzer
  let audioContext = null;
  let analyser = null;
  let microphone = null;
  let dataArray = null;
  let animationFrameId = null;
  let audioStream = null;
  let analyzerActive = false;

  // Cached greeting audio
  let cachedGreetingAudio = null;
  let isRecordingGreeting = false;
  let greetingRecorder = null;
  let greetingAudioChunks = [];
  let greetingPlayed = false;

  function startAudioAnalyzer(stream) {
    try {
      analyzerActive = true;
      audioStream = stream;
      audioContext = new (window.AudioContext || window.webkitAudioContext)();

      // Mobile Chrome requires AudioContext to be resumed after user interaction
      if (audioContext.state === "suspended") {
        audioContext.resume().catch((err) => {
          console.warn("Failed to resume audio context:", err);
        });
      }

      analyser = audioContext.createAnalyser();
      microphone = audioContext.createMediaStreamSource(stream);
      microphone.connect(analyser);

      analyser.fftSize = 256;
      const bufferLength = analyser.frequencyBinCount;
      dataArray = new Uint8Array(bufferLength);

      const levelMeter = document.getElementById("audio-level-meter");
      if (levelMeter) {
        levelMeter.style.display = "block";
      }

      function updateLevelMeter() {
        if (!analyser || !analyzerActive) {
          const levelMeter = document.getElementById("audio-level-meter");
          if (levelMeter) {
            levelMeter.style.display = "none";
          }
          return;
        }

        analyser.getByteFrequencyData(dataArray);

        // Calculate average volume level
        let sum = 0;
        for (let i = 0; i < dataArray.length; i++) {
          sum += dataArray[i];
        }
        const average = sum / dataArray.length;
        const normalizedLevel = Math.min(average / 128, 1); // Normalize to 0-1
        // Make 25% more sensitive by amplifying the level
        const sensitiveLevel = Math.min(normalizedLevel * 1.25, 1);

        // Dispatch custom event with audio level for gradient animation
        const audioLevelEvent = new CustomEvent("audioLevel", {
          detail: { level: sensitiveLevel, raw: average },
        });
        window.dispatchEvent(audioLevelEvent);

        // Update level bars
        const bars = document.querySelectorAll(".level-bar");
        bars.forEach((bar, index) => {
          // Create a wave effect with different thresholds for each bar
          const threshold = (index + 1) / bars.length;
          const barLevel = Math.max(0, sensitiveLevel - (1 - threshold));
          const height = Math.min(barLevel * 100, 100);
          bar.style.height = height + "%";
          bar.style.opacity = height > 5 ? 1 : 0.3;
        });

        animationFrameId = requestAnimationFrame(updateLevelMeter);
      }

      updateLevelMeter();
    } catch (error) {
      console.error("Error starting audio analyzer:", error);
    }
  }

  function stopAudioAnalyzer() {
    analyzerActive = false;

    if (animationFrameId) {
      cancelAnimationFrame(animationFrameId);
      animationFrameId = null;
    }

    if (microphone) {
      try {
        microphone.disconnect();
      } catch (e) {
        // Ignore
      }
      microphone = null;
    }

    if (audioContext && audioContext.state !== "closed") {
      audioContext.close().catch(() => {
        // Ignore errors
      });
      audioContext = null;
    }

    analyser = null;
    dataArray = null;

    const levelMeter = document.getElementById("audio-level-meter");
    if (levelMeter) {
      levelMeter.style.display = "none";
    }

    // Note: Don't stop the stream tracks here - Vapi needs them
    // The stream will be managed by Vapi
    audioStream = null;
  }

  // Load cached greeting from localStorage
  function loadCachedGreeting() {
    try {
      const cached = localStorage.getItem("vapi_greeting_audio");
      if (cached) {
        const audioBlob = new Blob([Uint8Array.from(JSON.parse(cached))], {
          type: "audio/webm",
        });
        cachedGreetingAudio = URL.createObjectURL(audioBlob);
        console.log("Loaded cached greeting from localStorage");
        return true;
      }
    } catch (error) {
      console.warn("Failed to load cached greeting:", error);
    }
    return false;
  }

  // Play cached greeting immediately
  function playCachedGreeting() {
    if (greetingPlayed) return; // Don't play twice in same session

    // Try to load from localStorage if not already loaded
    if (!cachedGreetingAudio) {
      loadCachedGreeting();
    }

    if (cachedGreetingAudio) {
      try {
        const audio = new Audio(cachedGreetingAudio);
        audio.volume = 0.8;
        audio.play().catch((error) => {
          console.warn("Failed to play cached greeting:", error);
        });
        greetingPlayed = true;
        console.log("Playing cached greeting immediately");
      } catch (error) {
        console.warn("Error playing cached greeting:", error);
      }
    }
  }

  // Start recording the greeting from Vapi
  function startRecordingGreeting(vapiInstance) {
    if (isRecordingGreeting || cachedGreetingAudio) return;

    try {
      isRecordingGreeting = true;
      greetingAudioChunks = [];

      // Try to get the audio stream from Vapi
      // Check if Vapi exposes the remote audio stream
      let remoteStream = null;

      // Try different methods to access Vapi's audio stream
      if (
        vapiInstance &&
        typeof vapiInstance.getRemoteAudioStream === "function"
      ) {
        remoteStream = vapiInstance.getRemoteAudioStream();
      } else if (vapiInstance && vapiInstance.remoteAudioStream) {
        remoteStream = vapiInstance.remoteAudioStream;
      } else if (
        vapiInstance &&
        vapiInstance.call &&
        vapiInstance.call.remoteAudioStream
      ) {
        remoteStream = vapiInstance.call.remoteAudioStream;
      }

      if (remoteStream) {
        greetingRecorder = new MediaRecorder(remoteStream, {
          mimeType: "audio/webm;codecs=opus",
        });

        greetingRecorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            greetingAudioChunks.push(event.data);
          }
        };

        greetingRecorder.onstop = () => {
          if (greetingAudioChunks.length > 0) {
            const audioBlob = new Blob(greetingAudioChunks, {
              type: "audio/webm",
            });
            cachedGreetingAudio = URL.createObjectURL(audioBlob);

            // Save to localStorage
            try {
              audioBlob.arrayBuffer().then((buffer) => {
                const uint8Array = new Uint8Array(buffer);
                localStorage.setItem(
                  "vapi_greeting_audio",
                  JSON.stringify(Array.from(uint8Array))
                );
                console.log("Cached greeting saved to localStorage");
              });
            } catch (error) {
              console.warn("Failed to save greeting to localStorage:", error);
            }
          }
          isRecordingGreeting = false;
        };

        // Record for up to 10 seconds (should be enough for greeting)
        greetingRecorder.start();
        console.log("Started recording greeting from Vapi");

        setTimeout(() => {
          if (greetingRecorder && greetingRecorder.state === "recording") {
            greetingRecorder.stop();
            console.log("Stopped recording greeting");
          }
        }, 10000);
      } else {
        console.log(
          "Vapi doesn't expose remote stream - cannot record greeting automatically"
        );
        console.log("Vapi instance methods:", Object.keys(vapiInstance || {}));
        isRecordingGreeting = false;
      }
    } catch (error) {
      console.warn("Failed to start recording greeting:", error);
      isRecordingGreeting = false;
    }
  }

  function initializeVapi() {
    const button = document.getElementById("vapi-voice-button");
    if (!button) return;

    let vapiInstance = null;
    let isActive = false;

    // Handle both click and touch events for mobile compatibility
    const handleStart = async function (e) {
      // Prevent double-firing on mobile
      if (e && e.type === "touchstart") {
        e.preventDefault();
      }
      if (isActive) {
        // Stop the call
        if (vapiInstance) {
          try {
            if (typeof vapiInstance.stop === "function") {
              vapiInstance.stop();
            } else if (typeof vapiInstance.end === "function") {
              vapiInstance.end();
            } else if (typeof vapiInstance.hangup === "function") {
              vapiInstance.hangup();
            }
          } catch (error) {
            console.error("Error stopping call:", error);
          }
          vapiInstance = null;
        }
        isActive = false;
        button.classList.remove("active");
        button.setAttribute("aria-label", "Start voice chat");
        // Hide microphone icon
        const micIcon = document.getElementById("mic-icon");
        if (micIcon) micIcon.style.display = "none";
        // Stop audio analyzer
        stopAudioAnalyzer();
      } else {
        // Start the call - lazy load SDK here
        try {
          // Show immediate visual feedback
          button.disabled = true;
          button.classList.add("connecting");

          // Show loading spinner
          const loadingSpinnerEl = document.getElementById("loading-spinner");
          if (loadingSpinnerEl) loadingSpinnerEl.style.display = "block";

          // Load SDK (should be instant if preloaded)
          console.log("Getting Vapi SDK (may already be preloaded)...");
          const Vapi = await loadVapiSDK();
          console.log("Vapi SDK loaded:", !!Vapi);
          console.log("Vapi type:", typeof Vapi);
          console.log("Vapi is function:", typeof Vapi === "function");
          console.log(
            "Vapi is constructor:",
            typeof Vapi === "function" && Vapi.prototype
          );

          if (!Vapi || typeof Vapi !== "function") {
            console.error("Vapi SDK validation failed");
            throw new Error("Vapi SDK failed to load or is not a constructor");
          }

          // Validate credentials before requesting microphone (do this in parallel)
          if (!publicKey || !assistantId) {
            console.error("Missing credentials:", {
              publicKey: !!publicKey,
              assistantId: !!assistantId,
            });
            throw new Error(
              "Vapi credentials are missing. Please set PUBLIC_VAPI_PUBLIC_KEY and PUBLIC_VAPI_ASSISTANT_ID."
            );
          }

          // Request microphone permission and initialize Vapi in parallel for speed
          const [stream] = await Promise.all([
            navigator.mediaDevices.getUserMedia({
              audio: {
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true,
              },
            }),
            // Small delay to ensure SDK is ready
            Promise.resolve(),
          ]);

          // Hide loading spinner, show microphone icon after permission granted
          const loadingSpinnerEl2 = document.getElementById("loading-spinner");
          const micIconEl = document.getElementById("mic-icon");
          if (loadingSpinnerEl2) loadingSpinnerEl2.style.display = "none";
          if (micIconEl) micIconEl.style.display = "block";

          // Activate toggle immediately after microphone permission granted
          button.classList.add("active");
          isActive = true;

          // Play cached greeting immediately if available
          playCachedGreeting();

          // Start audio level analyzer (non-blocking)
          try {
            startAudioAnalyzer(stream);
          } catch (analyzerError) {
            console.warn(
              "Audio analyzer failed (non-critical):",
              analyzerError
            );
            // Continue even if analyzer fails
          }

          // Validate credentials (already done above, but keeping for clarity)
          console.log("=== Vapi Initialization Debug ===");
          console.log("Step 1: Validating credentials");
          console.log("PublicKey present:", !!publicKey);
          console.log("PublicKey type:", typeof publicKey);
          console.log("PublicKey length:", publicKey ? publicKey.length : 0);
          console.log(
            "PublicKey preview:",
            publicKey
              ? `${publicKey.substring(0, 8)}...${publicKey.substring(publicKey.length - 4)}`
              : "N/A"
          );
          console.log("AssistantId present:", !!assistantId);
          console.log("AssistantId type:", typeof assistantId);
          console.log("AssistantId value:", assistantId);
          console.log(
            "AssistantId length:",
            assistantId ? assistantId.length : 0
          );

          if (!publicKey || !assistantId) {
            console.error("Missing credentials:", {
              publicKey: !!publicKey,
              assistantId: !!assistantId,
            });
            throw new Error(
              "Vapi credentials are missing. Please set PUBLIC_VAPI_PUBLIC_KEY and PUBLIC_VAPI_ASSISTANT_ID."
            );
          }

          // Additional validation - ensure assistantId is not just whitespace
          const trimmedAssistantId = assistantId.trim();
          if (
            typeof assistantId !== "string" ||
            trimmedAssistantId.length === 0
          ) {
            console.error("Invalid assistantId:", assistantId);
            throw new Error(
              "Assistant ID is invalid or empty. Please check your PUBLIC_VAPI_ASSISTANT_ID environment variable."
            );
          }
          console.log("Step 2: Credentials validated");
          console.log("Trimmed AssistantId:", trimmedAssistantId);
          console.log("Trimmed AssistantId length:", trimmedAssistantId.length);

          // Initialize Vapi
          try {
            // Create error handler that properly logs the error
            const handleError = function (error) {
              console.error("=== Vapi Error Handler ===");
              console.error("Vapi error event:", error);
              console.error("Error type:", typeof error);
              console.error("Error constructor:", error?.constructor?.name);

              // Try to extract error details
              let errorMessage = "Unknown error";
              if (error && typeof error === "object") {
                // Try to get message from nested error structure
                const nestedMessage = error.error?.message;
                if (nestedMessage && typeof nestedMessage === "object") {
                  errorMessage =
                    nestedMessage.message || JSON.stringify(nestedMessage);
                } else if (nestedMessage && typeof nestedMessage === "string") {
                  errorMessage = nestedMessage;
                } else {
                  errorMessage =
                    error.message || error.error || JSON.stringify(error);
                }

                // Log all error properties
                console.error("Error object properties:", Object.keys(error));
                console.error("Error object values:", error);

                // Check for specific error types
                if (error.type) {
                  console.error("Error type:", error.type);
                }
                if (error.stage) {
                  console.error("Error stage:", error.stage);
                }
                if (error.context) {
                  console.error("Error context:", error.context);
                }
                if (error.error) {
                  console.error("Nested error:", error.error);
                  if (typeof error.error === "object") {
                    console.error(
                      "Nested error keys:",
                      Object.keys(error.error)
                    );
                    if (error.error.message) {
                      console.error(
                        "Nested error message:",
                        error.error.message
                      );
                    }
                    if (error.error.status) {
                      console.error("Nested error status:", error.error.status);
                    }
                    if (error.error.statusText) {
                      console.error(
                        "Nested error statusText:",
                        error.error.statusText
                      );
                    }
                  }
                }
                if (error.message) {
                  console.error("Error message:", error.message);
                }
              } else if (error) {
                errorMessage = String(error);
              }

              console.error("Vapi error details summary:", {
                message: errorMessage,
                error: error,
                type: typeof error,
                keys:
                  error && typeof error === "object"
                    ? Object.keys(error)
                    : null,
              });

              // Check for 403/authentication errors
              const errorMessageStr = String(errorMessage);
              if (
                errorMessageStr.includes("403") ||
                errorMessageStr.includes("Forbidden") ||
                errorMessageStr.includes("doesn't allow origin")
              ) {
                console.error("=== 403 Forbidden Error Detected ===");
                console.error("This usually means:");
                console.error(
                  "1. The API key (publicKey) is invalid or expired"
                );
                console.error(
                  "2. The API key doesn't have permission to use this assistant"
                );
                console.error(
                  "3. The assistant ID doesn't belong to the account associated with the API key"
                );
                console.error(
                  "4. The API key might be a public key that needs different permissions"
                );
                console.error(
                  "PublicKey used:",
                  publicKey
                    ? `${publicKey.substring(0, 8)}...${publicKey.substring(publicKey.length - 4)}`
                    : "N/A"
                );
                console.error("AssistantId used:", assistantId);
              }

              isActive = false;
              button.classList.remove("active");
              button.setAttribute("aria-label", "Start voice chat");
              button.disabled = false;
              button.style.opacity = "1";

              // Hide microphone icon
              const micIcon = document.getElementById("mic-icon");
              if (micIcon) micIcon.style.display = "none";

              // Show user-friendly error
              alert("Voice chat error: " + errorMessage);

              if (vapiInstance) {
                try {
                  vapiInstance.stop?.();
                } catch (e) {
                  // Ignore cleanup errors
                }
                vapiInstance = null;
              }
            };

            // Vapi constructor takes: apiToken, apiBaseUrl, dailyCallConfig, dailyCallObject
            // Use empty config to avoid function cloning issues - we'll use event listeners instead
            console.log("Step 3: Creating Vapi instance");
            console.log("Vapi constructor:", typeof Vapi);
            console.log("Calling new Vapi() with:");
            console.log(
              "  - apiToken (publicKey):",
              publicKey
                ? `${publicKey.substring(0, 8)}...${publicKey.substring(publicKey.length - 4)}`
                : "N/A"
            );
            console.log("  - apiBaseUrl: undefined (using default)");
            console.log("  - dailyCallConfig: {}");

            vapiInstance = new Vapi(
              publicKey, // apiToken
              undefined, // apiBaseUrl (use default)
              {}, // Empty dailyCallConfig - functions can't be cloned by Daily.co
              undefined // dailyCallObject
            );

            console.log("Step 4: Vapi instance created");
            console.log("Vapi instance:", vapiInstance);
            console.log("Vapi instance type:", typeof vapiInstance);
            console.log(
              "Vapi instance methods:",
              vapiInstance
                ? Object.keys(vapiInstance).filter(
                    (key) => typeof vapiInstance[key] === "function"
                  )
                : []
            );
            console.log(
              "Has start method:",
              vapiInstance && typeof vapiInstance.start === "function"
            );
            console.log(
              "Has call method:",
              vapiInstance && typeof vapiInstance.call === "function"
            );
            console.log(
              "Has on method:",
              vapiInstance && typeof vapiInstance.on === "function"
            );

            // Use event listeners instead of callbacks to avoid cloning issues
            // Daily.co tries to clone functions in callbacks, causing DataCloneError
            if (vapiInstance && typeof vapiInstance.on === "function") {
              // Call start success event (fires when call actually starts)
              vapiInstance.on("call-start-success", function () {
                console.log("Vapi call started successfully");
                isActive = true;
                button.classList.add("active");
                button.setAttribute("aria-label", "End voice chat");
                button.disabled = false;
                button.style.opacity = "1";
                // Show microphone icon
                const micIcon = document.getElementById("mic-icon");
                if (micIcon) micIcon.style.display = "block";
              });

              // Also listen to call-start for immediate feedback
              vapiInstance.on("call-start", function () {
                console.log("Vapi call starting...");
                // Icon already switched before start() was called
                // Mark as active immediately
                isActive = true;
                button.classList.add("active");
                button.setAttribute("aria-label", "End voice chat");
                button.disabled = false;
                button.style.opacity = "1";
              });

              // Listen for when the call is fully connected and ready
              vapiInstance.on("call-update", function (data) {
                console.log("Vapi call update:", data);
                if (data && data.status) {
                  console.log("Call status:", data.status);
                  if (data.status === "connected" || data.status === "joined") {
                    isActive = true;
                    button.classList.add("active");
                    button.setAttribute("aria-label", "End voice chat");
                    button.disabled = false;
                    button.style.opacity = "1";
                  }
                }
              });

              // Listen for speech events to ensure it's actively listening/speaking
              vapiInstance.on("speech-start", function (data) {
                console.log("Vapi speech started:", data);
                isActive = true;
                button.classList.add("active");
              });

              vapiInstance.on("speech-end", function (data) {
                console.log("Vapi speech ended:", data);
              });

              vapiInstance.on("message", function (data) {
                console.log("Vapi message:", data);
                // Dispatch transcript event if message contains transcript data
                // Vapi message format: { role: "user" | "assistant", content: string }
                if (data) {
                  const text =
                    data.content ||
                    data.transcript ||
                    data.text ||
                    data.message;
                  const role = data.role || "assistant";
                  if (text && typeof text === "string" && text.trim()) {
                    const transcriptEvent = new CustomEvent("vapi-transcript", {
                      detail: {
                        text: text.trim(),
                        type: role === "user" ? "user" : "assistant",
                      },
                    });
                    window.dispatchEvent(transcriptEvent);
                  }
                }
              });

              // Listen for transcript events (if available in Vapi SDK)
              if (typeof vapiInstance.on === "function") {
                try {
                  vapiInstance.on("transcript", function (data) {
                    console.log("Vapi transcript event:", data);
                    if (data) {
                      const text = data.text || data.transcript || data.content;
                      const role = data.role || "assistant";
                      if (text && typeof text === "string" && text.trim()) {
                        const transcriptEvent = new CustomEvent(
                          "vapi-transcript",
                          {
                            detail: {
                              text: text.trim(),
                              type: role === "user" ? "user" : "assistant",
                            },
                          }
                        );
                        window.dispatchEvent(transcriptEvent);
                      }
                    }
                  });
                } catch (e) {
                  console.log("Transcript event not available:", e);
                }
              }

              // Listen for when the assistant starts speaking
              vapiInstance.on("assistant-speech-start", function (data) {
                console.log("Assistant started speaking:", data);
                isActive = true;
                button.classList.add("active");

                // Record the first greeting if not already cached
                if (
                  !greetingPlayed &&
                  !cachedGreetingAudio &&
                  !isRecordingGreeting
                ) {
                  startRecordingGreeting(vapiInstance);
                }
              });

              // Listen for assistant speech end to capture final transcript
              vapiInstance.on("assistant-speech-end", function (data) {
                console.log("Assistant speech ended:", data);
                if (data) {
                  const text = data.transcript || data.text || data.content;
                  if (text && typeof text === "string" && text.trim()) {
                    const transcriptEvent = new CustomEvent("vapi-transcript", {
                      detail: {
                        text: text.trim(),
                        type: "assistant",
                      },
                    });
                    window.dispatchEvent(transcriptEvent);
                  }
                }
              });

              // Listen for when user speech is detected
              vapiInstance.on("user-speech-start", function (data) {
                console.log("User speech detected:", data);
                isActive = true;
                button.classList.add("active");
              });

              // Listen for user speech end to capture final transcript
              vapiInstance.on("user-speech-end", function (data) {
                console.log("User speech ended:", data);
                if (data) {
                  const text = data.transcript || data.text || data.content;
                  if (text && typeof text === "string" && text.trim()) {
                    const transcriptEvent = new CustomEvent("vapi-transcript", {
                      detail: {
                        text: text.trim(),
                        type: "user",
                      },
                    });
                    window.dispatchEvent(transcriptEvent);
                  }
                }
              });

              // Call end event
              vapiInstance.on("call-end", function () {
                console.log("Vapi call ended");
                isActive = false;
                button.classList.remove("active");
                button.setAttribute("aria-label", "Start voice chat");
                button.disabled = false;
                button.style.opacity = "1";
                // Hide microphone icon
                const micIcon = document.getElementById("mic-icon");
                if (micIcon) micIcon.style.display = "none";
                // Stop audio analyzer
                stopAudioAnalyzer();
                // Stop recording greeting if still recording
                if (
                  greetingRecorder &&
                  greetingRecorder.state === "recording"
                ) {
                  greetingRecorder.stop();
                }
                // Reset greeting played flag for next session
                greetingPlayed = false;
                if (vapiInstance) {
                  vapiInstance = null;
                }
              });

              // Error event
              vapiInstance.on("error", handleError);
            }

            // Start the call with proper error handling
            // start() method signature: start(assistant, assistantOverrides, squad, workflow, workflowOverrides, options)
            try {
              // Show microphone icon before starting call
              const micIconEl = document.getElementById("mic-icon");
              if (micIconEl) micIconEl.style.display = "block";

              if (typeof vapiInstance.start === "function") {
                // Ensure assistantId is a valid string before calling start
                const validAssistantId =
                  assistantId && typeof assistantId === "string"
                    ? assistantId.trim()
                    : assistantId;

                console.log("=== Starting Vapi Call ===");
                console.log("Step 5: Preparing to call vapiInstance.start()");
                console.log("AssistantId validation:", {
                  original: assistantId,
                  trimmed: validAssistantId,
                  type: typeof validAssistantId,
                  length: validAssistantId?.length || 0,
                  isEmpty: !validAssistantId || validAssistantId === "",
                });

                if (!validAssistantId || validAssistantId === "") {
                  throw new Error(
                    "Assistant ID is empty or invalid. Value: " +
                      String(assistantId)
                  );
                }

                console.log("Step 6: Calling vapiInstance.start()");
                console.log("Method:", typeof vapiInstance.start);
                console.log("Parameter (assistantId):", validAssistantId);
                console.log("Parameter type:", typeof validAssistantId);
                console.log("Parameter length:", validAssistantId.length);
                console.log("Vapi instance before start:", vapiInstance);

                // Pass assistantId as the first parameter to start()
                try {
                  // Check current auth state before starting call
                  // Get fresh state from Clerk stores
                  let currentUserId = userId;
                  let currentIsAuthenticated = isAuthenticated;

                  // Get fresh auth state from Clerk stores
                  try {
                    const currentAuth = $authStore.get();
                    console.log(
                      "Current Clerk auth store at call start:",
                      currentAuth
                    );
                    if (currentAuth && currentAuth.userId) {
                      currentUserId = currentAuth.userId;
                      currentIsAuthenticated = true;
                    }

                    const currentUser = $userStore.get();
                    console.log(
                      "Current Clerk user store at call start:",
                      currentUser
                    );
                    if (currentUser && currentUser.id) {
                      currentUserId = currentUser.id;
                      currentIsAuthenticated = true;
                    }
                  } catch (e) {
                    console.warn("Error getting Clerk auth state:", e);
                  }

                  // #region agent log
                  fetch(
                    "http://127.0.0.1:7242/ingest/b70a058c-165e-4820-adda-384130e4a687",
                    {
                      method: "POST",
                      headers: { "Content-Type": "application/json" },
                      body: JSON.stringify({
                        location: "VoiceChatButton.astro:1165",
                        message: "Auth state check before Vapi start",
                        data: {
                          currentUserId: currentUserId || "not set",
                          currentIsAuthenticated,
                          authStoreHasUser:
                            $authStore.get()?.userId || "not set",
                          userStoreHasUser: $userStore.get()?.id || "not set",
                        },
                        timestamp: Date.now(),
                        sessionId: "debug-session",
                        runId: "init",
                        hypothesisId: "B",
                      }),
                    }
                  ).catch(() => {});
                  // #endregion

                  // Vapi start() method signature: start(assistant, assistantOverrides, squad, workflow, workflowOverrides, options)
                  // Build assistantOverrides with voice recognition settings if enabled
                  // According to Vapi docs, voice recognition should be nested under voiceRecognition object
                  // Add user context when authenticated
                  const finalAssistantOverrides = {
                    ...assistantOverrides,
                    ...(enableVoiceRecognition
                      ? {
                          voiceRecognition: {
                            enable: true,
                            speakerIdentification: true,
                            ...(voiceProfileId
                              ? { voiceProfileId: voiceProfileId }
                              : {}),
                            // Additional voice recognition settings can be added here
                          },
                        }
                      : {}),
                    // Add user context for authenticated users
                    ...(currentIsAuthenticated && currentUserId
                      ? {
                          // User context that can be accessed by the assistant
                          // This allows the assistant to know who is calling and provide personalized responses
                          user: {
                            id: currentUserId,
                            authenticated: true,
                          },
                          // You can add custom data here that the assistant can use
                          // For example, email access, tools, etc.
                        }
                      : {}),
                  };

                  console.log("Auth state at call start:", {
                    isAuthenticated: currentIsAuthenticated,
                    userId: currentUserId || "not set",
                    hasUserContext: currentIsAuthenticated && currentUserId,
                  });

                  // #region agent log
                  fetch(
                    "http://127.0.0.1:7242/ingest/b70a058c-165e-4820-adda-384130e4a687",
                    {
                      method: "POST",
                      headers: { "Content-Type": "application/json" },
                      body: JSON.stringify({
                        location: "VoiceChatButton.astro:1200",
                        message: "Auth state at call start",
                        data: {
                          isAuthenticated: currentIsAuthenticated,
                          userId: currentUserId || "not set",
                          hasUserContext:
                            currentIsAuthenticated && currentUserId,
                          finalOverridesHasUser:
                            currentIsAuthenticated &&
                            currentUserId &&
                            finalAssistantOverrides.user,
                        },
                        timestamp: Date.now(),
                        sessionId: "debug-session",
                        runId: "init",
                        hypothesisId: "F",
                      }),
                    }
                  ).catch(() => {});
                  // #endregion

                  console.log("Voice recognition config:", {
                    enabled: enableVoiceRecognition,
                    voiceProfileId: voiceProfileId || "not set",
                    assistantOverrides:
                      Object.keys(finalAssistantOverrides).length > 0
                        ? finalAssistantOverrides
                        : "none",
                  });

                  // #region agent log
                  fetch(
                    "http://127.0.0.1:7242/ingest/b70a058c-165e-4820-adda-384130e4a687",
                    {
                      method: "POST",
                      headers: { "Content-Type": "application/json" },
                      body: JSON.stringify({
                        location: "VoiceChatButton.astro:1055",
                        message: "Voice recognition config before start()",
                        data: {
                          enableVoiceRecognition,
                          voiceProfileId: voiceProfileId || "not set",
                          hasOverrides:
                            Object.keys(finalAssistantOverrides).length > 0,
                          overridesKeys: Object.keys(finalAssistantOverrides),
                        },
                        timestamp: Date.now(),
                        sessionId: "debug-session",
                        runId: "init",
                        hypothesisId: "B",
                      }),
                    }
                  ).catch(() => {});
                  // #endregion

                  // Pass assistantId and overrides to start()
                  // If no overrides, just pass assistantId (backward compatible)
                  const hasOverrides =
                    Object.keys(finalAssistantOverrides).length > 0;

                  // #region agent log
                  fetch(
                    "http://127.0.0.1:7242/ingest/b70a058c-165e-4820-adda-384130e4a687",
                    {
                      method: "POST",
                      headers: { "Content-Type": "application/json" },
                      body: JSON.stringify({
                        location: "VoiceChatButton.astro:1065",
                        message: "Calling vapiInstance.start()",
                        data: {
                          hasOverrides,
                          overrides: hasOverrides
                            ? finalAssistantOverrides
                            : null,
                          assistantId: validAssistantId,
                        },
                        timestamp: Date.now(),
                        sessionId: "debug-session",
                        runId: "init",
                        hypothesisId: "C",
                      }),
                    }
                  ).catch(() => {});
                  // #endregion

                  const startResult = hasOverrides
                    ? vapiInstance.start(
                        validAssistantId,
                        finalAssistantOverrides
                      )
                    : vapiInstance.start(validAssistantId);

                  // #region agent log
                  fetch(
                    "http://127.0.0.1:7242/ingest/b70a058c-165e-4820-adda-384130e4a687",
                    {
                      method: "POST",
                      headers: { "Content-Type": "application/json" },
                      body: JSON.stringify({
                        location: "VoiceChatButton.astro:1075",
                        message: "vapiInstance.start() called",
                        data: {
                          hasOverrides,
                          resultType: typeof startResult,
                          isPromise:
                            startResult &&
                            typeof startResult.then === "function",
                        },
                        timestamp: Date.now(),
                        sessionId: "debug-session",
                        runId: "init",
                        hypothesisId: "D",
                      }),
                    }
                  ).catch(() => {});
                  // #endregion

                  console.log(
                    "Step 7: vapiInstance.start() called successfully"
                  );
                  console.log("Start result:", startResult);
                  console.log("Start result type:", typeof startResult);

                  // Keep connecting state until call actually starts
                  // Don't mark as active yet - wait for call-start-success event
                  button.setAttribute("aria-label", "Connecting...");

                  if (startResult && typeof startResult.then === "function") {
                    console.log("Start returned a Promise");
                    startResult
                      .then((result) => {
                        console.log("Start Promise resolved:", result);
                        // Ensure active state is set
                        isActive = true;
                        button.classList.add("active");
                      })
                      .catch((err) => {
                        console.error("Start Promise rejected:", err);
                      });
                  }
                } catch (startCallError) {
                  console.error(
                    "Error thrown by vapiInstance.start():",
                    startCallError
                  );
                  console.error("Error name:", startCallError?.name);
                  console.error("Error message:", startCallError?.message);
                  console.error("Error stack:", startCallError?.stack);
                  throw startCallError;
                }
              } else if (typeof vapiInstance.call === "function") {
                console.log("Calling Vapi...");
                if (!assistantId || assistantId.trim() === "") {
                  throw new Error("Assistant ID is empty or invalid");
                }
                vapiInstance.call(assistantId.trim());
              } else {
                throw new Error(
                  "Vapi instance does not have start() or call() method"
                );
              }
            } catch (startError) {
              console.error("Error starting Vapi call:", startError);
              // Hide microphone icon on error
              const micIconEl = document.getElementById("mic-icon");
              if (micIconEl) micIconEl.style.display = "none";
              handleError(startError);
              throw startError;
            }
          } catch (initError) {
            console.error("Vapi initialization error:", initError);
            button.disabled = false;
            button.classList.remove("connecting");
            button.classList.remove("active");
            isActive = false;
            button.style.opacity = "1";
            // Hide loading spinner and microphone icon
            const loadingSpinnerErr =
              document.getElementById("loading-spinner");
            const micIconErr = document.getElementById("mic-icon");
            if (loadingSpinnerErr) loadingSpinnerErr.style.display = "none";
            if (micIconErr) micIconErr.style.display = "none";
            // Stop audio analyzer if it was started
            stopAudioAnalyzer();
            alert(
              "Failed to initialize voice chat: " +
                (initError.message || "Unknown error")
            );
          }
        } catch (error) {
          console.error("Error:", error);
          button.disabled = false;
          button.classList.remove("connecting");
          button.classList.remove("active");
          isActive = false;
          button.style.opacity = "1";
          // Hide loading spinner and microphone icon
          const loadingSpinner = document.getElementById("loading-spinner");
          const micIcon = document.getElementById("mic-icon");
          if (loadingSpinner) loadingSpinner.style.display = "none";
          if (micIcon) micIcon.style.display = "none";
          // Stop audio analyzer on error
          stopAudioAnalyzer();

          if (error.name === "NotAllowedError") {
            alert("Please allow microphone access to use voice chat.");
          } else if (error.message && error.message.includes("constructor")) {
            alert(
              "Voice chat is temporarily unavailable. Please refresh the page."
            );
          } else {
            alert(
              "Error accessing microphone. Please check your browser settings."
            );
          }
        }
      }
    };

    // Add both click and touchstart listeners for mobile compatibility
    button.addEventListener("click", handleStart);
    button.addEventListener("touchstart", handleStart, { passive: false });
  }

  // Preload SDK immediately if credentials are available
  // This reduces the delay when user clicks the button
  if (publicKey && assistantId) {
    // Preload SDK in the background
    loadVapiSDK().catch((error) => {
      console.warn("Failed to preload Vapi SDK:", error);
      // Don't show error to user - it will retry on click
    });

    // Load cached greeting if available
    loadCachedGreeting();

    // Initialize button handler
    initializeVapi();
  } else {
    // Disable button if credentials are missing
    const button = document.getElementById("vapi-voice-button");
    if (button) {
      button.style.opacity = "0.5";
      button.style.cursor = "not-allowed";
      button.disabled = true;
      button.setAttribute(
        "title",
        "Voice chat unavailable - credentials missing"
      );
    }
  }
</script>

<style>
  .vapi-widget-container {
    position: fixed;
    z-index: 9999;
    opacity: 0;
    animation: fadeIn 0.5s ease-in-out 1.5s forwards;
  }

  @keyframes fadeIn {
    from {
      opacity: 0;
    }
    to {
      opacity: 1;
    }
  }

  /* Position classes - using explicit CSS since Tailwind is not configured */
  .vapi-widget-container.bottom-right {
    bottom: 24px;
    right: 24px;
  }

  .vapi-widget-container.bottom-left {
    bottom: 24px;
    left: 24px;
  }

  .vapi-widget-container.top-right {
    top: 24px;
    right: 24px;
  }

  .vapi-widget-container.top-left {
    top: 24px;
    left: 24px;
  }

  .vapi-widget-container.center-bottom {
    bottom: 80px;
    left: 50%;
    transform: translateX(-50%);
  }

  .vapi-voice-button {
    position: relative;
    width: 90px; /* Made wider */
    height: 40px;
    border-radius: 20px;
    background: transparent;
    border: 2px solid transparent; /* Transparent to show gradient border */
    cursor: pointer;
    display: flex;
    align-items: center;
    padding: 4px;
    box-shadow: 0 2px 10px rgba(0, 0, 0, 0.3);
    transition: all 0.3s ease;
    overflow: hidden;
  }

  /* Colorful gradient border using pseudo-element */
  .vapi-voice-button::before {
    content: "";
    position: absolute;
    inset: -4px; /* Extend 4px outside for wider border effect */
    border-radius: 22px; /* Slightly larger to match wider border */
    background: linear-gradient(
      90deg,
      #ff0000 0%,
      #ffff00 25%,
      #00ffff 50%,
      #ff00ff 75%,
      #ff0000 100%
    );
    z-index: -1; /* Behind the button content */
    opacity: 0.8;
    transition: opacity 0.3s ease;
  }

  .vapi-voice-button.active::before {
    opacity: 1;
  }

  .vapi-widget-container.center-bottom .vapi-voice-button:hover {
    transform: scale(1.05);
    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.4);
  }

  .vapi-widget-container.center-bottom .vapi-voice-button:hover::before {
    opacity: 1;
  }

  .vapi-widget-container.center-bottom .vapi-voice-button:active {
    transform: scale(0.98);
  }

  /* Toggle track - gradient background */
  .toggle-track {
    position: absolute;
    inset: 3px; /* Contracted from edges to show more border */
    border-radius: 17px; /* Adjusted to match inset */
    background: #000000; /* Black background when off */
    opacity: 1;
    transition:
      background 0.3s ease,
      opacity 0.3s ease;
  }

  /* Colorful gradient border using pseudo-element */
  .toggle-track::before {
    content: "";
    position: absolute;
    inset: -4px; /* Extend 4px outside for wider border effect */
    border-radius: 22px; /* Slightly larger to match wider border */
    background: linear-gradient(
      90deg,
      #ff0000 0%,
      #ffff00 25%,
      #00ffff 50%,
      #ff00ff 75%,
      #ff0000 100%
    );
    z-index: -1; /* Behind the black background */
    opacity: 0.8;
    transition: opacity 0.3s ease;
  }

  .vapi-voice-button.active .toggle-track {
    background:
      radial-gradient(circle at 30% 30%, #ff0000 0%, transparent 40%),
      radial-gradient(circle at 70% 30%, #ff00ff 0%, transparent 40%),
      radial-gradient(circle at 50% 50%, #00ffff 0%, transparent 40%),
      radial-gradient(circle at 30% 70%, #00ff00 0%, transparent 40%),
      radial-gradient(circle at 70% 70%, #ffff00 0%, transparent 40%),
      radial-gradient(
        ellipse at center,
        #ff0000 0%,
        #ffff00 25%,
        #00ffff 50%,
        #ff00ff 75%,
        #ff0000 100%
      );
    background-size: 60px 40px;
    background-repeat: repeat;
    opacity: 1;
  }

  /* Brighter border when active */
  .vapi-voice-button.active .toggle-track::before {
    opacity: 1;
  }

  /* Ensure inactive state returns to black background */
  .vapi-voice-button:not(.active) .toggle-track {
    background: #000000;
  }

  .vapi-voice-button:not(.active) .toggle-track::before {
    opacity: 0.8;
  }

  .vapi-voice-button.connecting .toggle-track {
    opacity: 0.8;
    animation: connecting-pulse 1s cubic-bezier(0.4, 0, 0.6, 1) infinite;
  }

  /* Toggle thumb - sliding circle */
  .toggle-thumb {
    position: absolute;
    left: 2px;
    top: 2px; /* Moved up 2px (was 4px) */
    width: 32px;
    height: 32px;
    border-radius: 50%;
    background: rgba(255, 255, 255, 0.9);
    display: flex;
    align-items: center;
    justify-content: center;
    transition: transform 0.3s cubic-bezier(0.4, 0, 0.2, 1);
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.3);
    z-index: 2;
  }

  .vapi-voice-button.active .toggle-thumb {
    transform: translateX(50px); /* Adjusted for wider button (was 40px) */
    background: rgba(255, 255, 255, 1);
  }

  .vapi-voice-button.active {
    border-color: rgba(255, 255, 255, 0.6);
  }

  /* Loading spinner */
  .loading-spinner {
    position: absolute;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
    width: 24px;
    height: 24px;
    z-index: 2;
  }

  .spinner-ring {
    width: 100%;
    height: 100%;
    border: 3px solid rgba(255, 255, 255, 0.3);
    border-top-color: rgba(255, 255, 255, 1);
    border-radius: 50%;
    animation: spin 0.8s linear infinite;
  }

  @keyframes spin {
    to {
      transform: rotate(360deg);
    }
  }

  @keyframes connecting-pulse {
    0%,
    100% {
      opacity: 0.8;
    }
    50% {
      opacity: 1;
    }
  }

  .vapi-icon {
    width: 20px;
    height: 20px;
    color: #000;
    z-index: 3;
    position: relative;
    transition: opacity 0.2s ease;
    pointer-events: none;
  }

  /* Icon state management */
  .icon-active {
    display: none;
  }

  .vapi-voice-button.active .icon-active {
    display: block;
    animation: mic-pulse 1.5s ease-in-out infinite;
  }

  @keyframes mic-pulse {
    0%,
    100% {
      transform: scale(1);
      opacity: 1;
    }
    50% {
      transform: scale(1.1);
      opacity: 0.8;
    }
  }

  /* Audio level meter */
  .audio-level-meter {
    position: absolute;
    bottom: -40px;
    left: 50%;
    transform: translateX(-50%);
    display: flex;
    align-items: flex-end;
    justify-content: center;
    gap: 3px;
    height: 30px;
    width: 100%;
    pointer-events: none;
  }

  .level-bars {
    display: flex;
    align-items: flex-end;
    justify-content: center;
    gap: 3px;
    height: 100%;
  }

  .level-bar {
    width: 4px;
    min-height: 4px;
    background: linear-gradient(
      to top,
      rgba(102, 126, 234, 1) 0%,
      rgba(102, 126, 234, 0.8) 50%,
      rgba(255, 255, 255, 0.6) 100%
    );
    border-radius: 2px;
    transition:
      height 0.1s ease,
      opacity 0.2s ease;
    opacity: 0.3;
    height: 4px;
  }

  .vapi-voice-button.active ~ .audio-level-meter .level-bar {
    background: linear-gradient(
      to top,
      #00ffff 0%,
      #ff00ff 50%,
      rgba(255, 255, 255, 0.8) 100%
    );
  }

  /* Responsive adjustments */
  @media (max-width: 768px) {
    .vapi-widget-container.center-bottom {
      bottom: 60px;
    }

    .vapi-voice-button {
      width: 70px;
      height: 36px;
    }

    .toggle-thumb {
      width: 28px;
      height: 28px;
    }

    .vapi-voice-button.active .toggle-thumb {
      transform: translateX(34px);
    }

    .vapi-icon {
      width: 18px;
      height: 18px;
    }

    .audio-level-meter {
      bottom: -35px;
    }

    .level-bar {
      width: 3px;
      gap: 2px;
    }
  }
</style>
